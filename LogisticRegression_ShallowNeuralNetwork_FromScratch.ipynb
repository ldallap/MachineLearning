{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5cfbd8b",
   "metadata": {},
   "source": [
    "# Neural Network with Logistic Regressio, from shallow to deep\n",
    "\n",
    "<a name='1'></a>\n",
    "\n",
    "Let's build a shallow neural network (NN), i.e., without hidden layer.\n",
    "\n",
    "We will build it from scrath and use skilearn only to deal with data split and normalization.\n",
    "\n",
    "In the end, we will build the same NN using PyTorch and Tensorflow.\n",
    "\n",
    "\n",
    "**Shallow Neural Network representation**\n",
    "\n",
    "<img src=\"Images_Notebooks/ShallowNeuralNet.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "\n",
    "**Key steps**:\n",
    "    - Initialize the weights (same dimension as the input data, i.e, number of features)\\\n",
    "    - Apply linear transformation: $Z = W\\cdot X + b$, where W= weights, X=input data and b = bias \\\n",
    "    - Pass Z through a sigmoid activation function $g(z) = \\sigma(Z) = \\frac{1}{1 + exp^{-Z}}$\\\n",
    "    - Compute the Cost Function $C = \\frac{-1}{m} \\sum_{i=1}^{m} [Y log(A) - (1-Y)log(1-A)]$, where Y is the real label of the data\\\n",
    "    - Compute Gradient Descent (GD) and update the weights.\\\n",
    "    - Repeat the process as many times wanted (# of iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67bc72",
   "metadata": {},
   "source": [
    "##### RESOURCES:\n",
    "\n",
    "1) Coursera: Neural Networks and Deep Learning\\\n",
    "2) https://www.youtube.com/watch?v=w8yWXqWQYmU&t=664s&ab_channel=SamsonZhang \\\n",
    "3) https://medium.com/@jacobbumgarner/breaking-it-down-logistic-regression-e5c3f1450bd#cee3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98ed267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Logistic Regression\n",
    "    Args:\n",
    "        n_input_features (int): # of features in the dataset\n",
    "    Attirbutes:\n",
    "        weights (np.ndarray)\n",
    "        bias (float)\n",
    "        fit (bool): Whether the model has been fit or not to training data. Default: False\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_input_features: int):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Initialize weights (W) and bias (b)    \n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(n_input_features,1)*0.01 # Do not start with zeros. Font: Andrew NG course\n",
    "        self.bias = np.zeros((1,1))\n",
    "        \n",
    "        self.fit = False # indicates the training state of the classifier\n",
    "        \n",
    "    \n",
    "    def linear_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Linear component\n",
    "            Z = W X + b\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input data\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: transformed data Z\n",
    "        \n",
    "        W -> weights\n",
    "        b -> bias\n",
    "        X -> Input data\n",
    "        \"\"\"\n",
    "        return np.matmul(X, self.weights) + self.bias # np.matmul = matrix multiplication line vs colunm\n",
    "    \n",
    "    def sigmoid(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Sigmoid function\n",
    "            sigma(z) = 1 / (1 + exp(-z))\n",
    "        \n",
    "        Args:\n",
    "            Z (np.ndarray): Linear transformed data\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Data evaluated in a sigmoid function\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-Z))\n",
    "        \n",
    "    \n",
    "    def cost_cross_entropy(self, A: np.ndarray, Y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Cross-Entropy Cost Function\n",
    "            L(Y,A) = (1/m) \\sum_n [(-Y log(A)) - (1-Y)(log(1-A))]\n",
    "            \n",
    "        Args:\n",
    "            Y (np.ndarray): true label of the data\n",
    "            A (np.ndarray): label \"probability\" \n",
    "        \"\"\"\n",
    "        m = Y.shape[0]\n",
    "        epsilon = 1e-6\n",
    "        \n",
    "        cost = (-1/m) * np.sum( Y*np.log(A)  + \\\n",
    "                (1 - Y) * np.log(1-A+epsilon))\n",
    "        \n",
    "        # cost = np.squeeze(cost) make sure cost is in the correct shape (turn [[1]] into 1)\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def gradient_descent(self, X: np.ndarray, A: np.ndarray, Y: np.ndarray,\n",
    "                        learning_rate = 0.01) -> None:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Compute the Gradient Descent\n",
    "            dZ = (A - Y)\n",
    "            dW = dZ . X \n",
    "            dB = dZ\n",
    "                and update\n",
    "            W = W - dW * learning_rate\n",
    "            b = b - dB * learning_rate\n",
    "        \"\"\"\n",
    "        m = A.shape[0]\n",
    "        oneover_m = 1./m\n",
    "        \n",
    "        dZ = (A - Y)\n",
    "        dW = oneover_m * np.sum(dZ * X, axis=0, keepdims=True).T\n",
    "        dB = oneover_m * np.sum(dZ, axis=0, keepdims=True).T\n",
    "        \n",
    "        # Update\n",
    "        self.weights -= dW * learning_rate\n",
    "        self.bias -= dB * learning_rate\n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def train(self, X: np.ndarray, Y: np.ndarray,\n",
    "             epochs: int = 100, learning_rate: float = 0.01, batch_size: int = 10,\n",
    "             verbose: bool = False) -> np.ndarray:\n",
    "        \n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Fit the logistic regression model to training data\n",
    "            Use minibatch GD.\n",
    "            \n",
    "        Args:\n",
    "            X (np.ndarray): Training dataset\n",
    "            Y (np.ndarray): Training targets\n",
    "            epochs (int, optional; default = 100): Number of iterations\n",
    "            learning_rate (float, optional; default = 0.01): Learning rate step size\n",
    "            batch_size (int, optional; default = 10): Size of batch for GD\n",
    "            verbose (bool, optional; default = False): __description__\n",
    "        \n",
    "        Raises:\n",
    "            Attribute: Raises error if the model is already fitted\n",
    "            ValueError: Raises error if the number of features dosen't match the instantiated feature count.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: The cost history\n",
    "        \n",
    "        \"\"\"\n",
    "        # Raise flags\n",
    "        if self.fit:\n",
    "            raise AttributeError(\"Error: Model already fitted\")\n",
    "        self.fit = True\n",
    "        \n",
    "        if not X.shape[-1] == self.weights.shape[0]:\n",
    "            raise ValueError(\"Shape of X is different from Weights\")\n",
    "        \n",
    "        if Y.ndim == 1:\n",
    "            Y = np.expand_dims(Y, axis=1)\n",
    "        \n",
    "        # Fit the model\n",
    "        cost_hist = []\n",
    "        accuracies = []\n",
    "        weight_hist = []\n",
    "        bias_hist = []\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            weight_hist.append(self.weights[:,0].copy())\n",
    "            bias_hist.append(self.bias.copy())\n",
    "            \n",
    "            \n",
    "            if batch_size:\n",
    "                batch_indices = np.random.choice(\n",
    "                    X.shape[0], size = batch_size, replace = False\n",
    "                )\n",
    "                X_batch, Y_batch = X[batch_indices], Y[batch_indices]\n",
    "            else:\n",
    "                X_batch, Y_batch = X, Y\n",
    "                \n",
    "            \n",
    "            # Linear Transformation\n",
    "            Z = self.linear_transform(X_batch)\n",
    "            \n",
    "            # Sigmoid activation\n",
    "            A = self.sigmoid(Z)\n",
    "            \n",
    "            # Cost function (Cross- Entropy)\n",
    "            cost = self.cost_cross_entropy(A, Y_batch)\n",
    "            \n",
    "            # Perform GD\n",
    "            self.gradient_descent(X_batch, A, Y_batch, learning_rate = learning_rate)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Epoch: {_}, Cost: {cost: 0.3f}                  ', end='\\r')\n",
    "                \n",
    "            cost_hist.append(cost)\n",
    "            accuracies.append(self.accuracy(self.predict(X), Y[:,0]))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Final cost: {cost:0.2f}                  ')\n",
    "            \n",
    "        self.fit = True\n",
    "        \n",
    "        return np.array(cost_hist), np.array(accuracies), np.array(weight_hist).T, np.array(bias_hist).T[0,0]\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Predict the labels\n",
    "            \n",
    "        Args:\n",
    "            X (np.ndarray): Data for predictions\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Prediction for each sample\n",
    "        \"\"\"\n",
    "        if not self.fit:\n",
    "            raise AttributeError(\"Error: This classifier is not trained\")\n",
    "        \n",
    "        Z = self.linear_transform(X)\n",
    "        A = self.sigmoid(Z)\n",
    "        \n",
    "        return A.T[0]\n",
    "    \n",
    "    def accuracy(self, predictions: np.ndarray, labels: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Accuracy of prediction\n",
    "            \n",
    "        Args:\n",
    "            predictions (np.ndarray): predict (function) output\n",
    "            labels (np.ndarray): True labels\n",
    "            \n",
    "        Returns:\n",
    "            float: prediction accuracy\n",
    "        \"\"\"\n",
    "        \n",
    "        overlap = (predictions >= 0.5) == labels\n",
    "        accuracy = (overlap.sum() / predictions.shape[0]) * 100 # convert to probability\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6eab5d",
   "metadata": {},
   "source": [
    "Load data and standarize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d31c985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"Dataset/heart.csv\")\n",
    "df.target = df.target.replace({0: 1, 1: 0}) # Target are 0=yes and 1=no, let's change\n",
    "targets = df.pop(\"target\")\n",
    "\n",
    "\n",
    "# Split data into Training and Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df, targets, test_size = 0.25, random_state = 42\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "features_to_standardize = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [(\"scaler\", StandardScaler(), features_to_standardize)], remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "x_train = column_transformer.fit_transform(x_train)\n",
    "x_test = column_transformer.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf99000",
   "metadata": {},
   "source": [
    "Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_input_features=x_train.shape[-1])\n",
    "\n",
    "costs, accuracies, weights, bias = model.train(x_train, y_train,\n",
    "                                              epochs = 5000,\n",
    "                                              learning_rate=0.01,\n",
    "                                              batch_size=None,\n",
    "                                              verbose=False)\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "accuracy = model.accuracy(predictions, y_test)\n",
    "\n",
    "print(f\"Model test accuracy: {accuracy:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada490a5",
   "metadata": {},
   "source": [
    "## Same thing using Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "x_train2 = tf.convert_to_tensor(x_train)\n",
    "y_train2 = tf.convert_to_tensor(y_train)\n",
    "x_test2 = tf.convert_to_tensor(x_test)\n",
    "y_test2 = tf.convert_to_tensor(y_test)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensors((x_train2, y_train2)) #.batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensors((x_test2, y_test2)) #.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225cb500",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(\n",
    "        1,\n",
    "        kernel_initializer = \"HeNormal\",\n",
    "        bias_initializer = \"zeros\",\n",
    "        activation = \"sigmoid\"\n",
    "    )\n",
    "])\n",
    "\n",
    "model.compile(loss = \"bce\", metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, epochs = 5000, verbose = 0)\n",
    "\n",
    "results = model.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Model test accuracy: {results}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac38d8",
   "metadata": {},
   "source": [
    "## Same thing using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e18f2c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import from_numpy\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a718a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_input_features = x_train.shape[-1]\n",
    "\n",
    "x_train2, x_test2 = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
    "y_train2, y_test2 = torch.from_numpy((y_train.astype(float)).to_numpy()), torch.from_numpy((y_test.astype(float)).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e37547b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ShallowNN, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = torch.sigmoid(self.linear(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc436e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "input_dim = n_input_features\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = ShallowNN(input_dim, output_dim)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9770a098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|████████████████████| 5000/5000 [00:01<00:00, 3285.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for epochs in tqdm(range(int(n_epochs)), desc = 'Training Epochs'):\n",
    "    x = x_train2\n",
    "    labels = y_train2\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x_train2.float())\n",
    "    loss = criterion(torch.squeeze(outputs.float()), labels.float())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace718f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
